{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rPjoKgg6y622",
        "Mtvem5WT6SUU",
        "8cwEZn-_7FW-",
        "llR6dkJV77lM",
        "75ZRWL3_9ivY",
        "pjz5Puv3DBWH",
        "1S7wZsy2I2rn",
        "PAVY2RdcMEr9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# install dependencies"
      ],
      "metadata": {
        "id": "rPjoKgg6y622"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a5_qVouFxufq"
      },
      "outputs": [],
      "source": [
        "! pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install omegaconf"
      ],
      "metadata": {
        "id": "S8DjAph59L5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "metadata": {
        "id": "rWRP2hxK12x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "metadata": {
        "id": "yuMAZgQq2MC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd models/research"
      ],
      "metadata": {
        "id": "jv3s9mUG2bKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!protoc object_detection/protos/*.proto --python_out=."
      ],
      "metadata": {
        "id": "QnNXrQGF2ms3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/cocodataset/cocoapi.git"
      ],
      "metadata": {
        "id": "vSfvl-il2nGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd cocoapi/PythonAPI"
      ],
      "metadata": {
        "id": "rxOgxriq2p07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "metadata": {
        "id": "9ZRGJ6Ew26tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp -r pycocotools /content/models/research"
      ],
      "metadata": {
        "id": "djNcy0Pd3B01"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "id": "KirPAUll3JF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "id": "NI-1s6p-3Lfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cp object_detection/packages/tf2/setup.py ."
      ],
      "metadata": {
        "id": "r60CLziu3MuE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install ."
      ],
      "metadata": {
        "id": "mzT7vdhQ3GZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python object_detection/builders/model_builder_tf2_test.py"
      ],
      "metadata": {
        "id": "I-9LEfWY3SJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "id": "aYM-Uof23mFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "id": "bGwgAui43w-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5"
      ],
      "metadata": {
        "id": "iq6dphKx3zyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ensemble-boxes"
      ],
      "metadata": {
        "id": "wsjeiPaACsBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the Pipeline"
      ],
      "metadata": {
        "id": "I1MP3Rg34HlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASSES_MAP = {0:'wheel_mark',\n",
        "               1:'equal_interval',\n",
        "               2:'pavement',\n",
        "               3:'pothole'}\n",
        "\n",
        "SEVERITY_CLASSES_MAP = {0:'crack',\n",
        "                        1:'insignificant',\n",
        "                        2:'serious',\n",
        "                        3:'tolerable'}"
      ],
      "metadata": {
        "id": "nwvFX_CnNvAK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Image"
      ],
      "metadata": {
        "id": "Mtvem5WT6SUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2"
      ],
      "metadata": {
        "id": "6jjf4L4j6VMR"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img = Image.open(\"path_to_test_image.jpg\")\n",
        "img_np = np.array(img)"
      ],
      "metadata": {
        "id": "gQSrGZU-6YhG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_np = cv2.imread(\"path_to_test_image.jpg\")\n",
        "image_rgb = cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n",
        "image_expanded = np.expand_dims(image_rgb, axis=0)\n",
        "\n",
        "# The input needs to be a tensor, convert it using `tf.convert_to_tensor`.\n",
        "input_tensor = tf.convert_to_tensor(img_np)\n",
        "# The model expects a batch of images, so add an axis with `tf.newaxis`.\n",
        "input_tensor = input_tensor[tf.newaxis, ...]"
      ],
      "metadata": {
        "id": "aJMmf1yXAcmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load YOLO V8"
      ],
      "metadata": {
        "id": "8cwEZn-_7FW-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "id": "VjW7aOZF4SOG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load yolov8 model\n",
        "yolov8 = YOLO('v8.pt')"
      ],
      "metadata": {
        "id": "z3T_w1aN4aMV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load YOLO V5"
      ],
      "metadata": {
        "id": "llR6dkJV77lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "U4ACd0G479RC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd yolov5"
      ],
      "metadata": {
        "id": "9MrIZQH59QN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolov5 = torch.hub.load('ultralytics/yolov5', 'custom', path='../v5.pt')"
      ],
      "metadata": {
        "id": "UauTQxn28Ylq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xH3sYAK8utw",
        "outputId": "71ba6821-057c-4b28-8ad6-01f548e35b4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Faster RCNN Models"
      ],
      "metadata": {
        "id": "75ZRWL3_9ivY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging (1)\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import argparse\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# Enable GPU dynamic memory allocation\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# PROVIDE PATH TO MODEL DIRECTORY\n",
        "PATH_TO_MODEL_DIR = '/content/FRCNN'  # Modify as needed\n",
        "\n",
        "\n",
        "# PROVIDE PATH TO LABEL MAP\n",
        "PATH_TO_LABELS = '/content/FRCNN/label_map.pbtxt'  # Modify as needed\n",
        "\n",
        "# PROVIDE THE MINIMUM CONFIDENCE THRESHOLD\n",
        "MIN_CONF_THRESH = float(0.40)\n",
        "\n",
        "# LOAD THE MODELS\n",
        "\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils as viz_utils\n",
        "\n",
        "PATH_TO_SAVED_MODEL1 = PATH_TO_MODEL_DIR + \"/FRCNN1\"\n",
        "PATH_TO_SAVED_MODEL2 = PATH_TO_MODEL_DIR + \"/FRCNN2\"\n",
        "\n",
        "# LOAD SAVED MODEL AND BUILD DETECTION FUNCTION\n",
        "detect_fn1 = tf.saved_model.load(PATH_TO_SAVED_MODEL1)\n",
        "detect_fn2 = tf.saved_model.load(PATH_TO_SAVED_MODEL2)\n"
      ],
      "metadata": {
        "id": "bpA2gF9Y9ugG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Prediction Functions"
      ],
      "metadata": {
        "id": "pjz5Puv3DBWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def yolov8_prediction():\n",
        "  results = yolov8(img)\n",
        "  results = results[0]\n",
        "  results = results.cpu()\n",
        "  boxes = results.boxes.xyxyn.tolist()  # box with xyxy format but normalized, (N, 4)\n",
        "  scores = results.boxes.conf.tolist()   # confidence score, (N, 1)\n",
        "  classes = results.boxes.cls.tolist()    # cls, (N, 1)\n",
        "  boxes = [[box[1], box[0], box[3], box[2]] for box in boxes]  # convert to yxyx format\n",
        "  return boxes, scores, classes"
      ],
      "metadata": {
        "id": "t-P9lZuLBrSH"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def yolov5_prediction():\n",
        "  results = yolov5(img)\n",
        "  results = results.pandas().xyxy[0]\n",
        "\n",
        "  h, w = img_np.shape[0], img_np.shape[1]\n",
        "\n",
        "  results['ymin'] = results['ymin'] / h\n",
        "  results['ymax'] = results['ymax'] / h\n",
        "  results['xmin'] = results['xmin'] / w\n",
        "  results['xmax'] = results['xmax'] / w\n",
        "\n",
        "  boxes = results[['ymin', 'xmin', 'ymax', 'xmax']].values.tolist()\n",
        "  scores = results.confidence.values.tolist()\n",
        "  classes = results['class'].values.tolist()\n",
        "\n",
        "  detectionsDect = {'boxes': boxes, 'scores': scores, 'classes': classes}\n",
        "\n",
        "  return boxes, scores, classes"
      ],
      "metadata": {
        "id": "lkbrH_THBrUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frcnn_1_prediction():\n",
        "  # input_tensor = np.expand_dims(image_np, 0)\n",
        "  FRCNN_1_detections = detect_fn1(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "\n",
        "  num_detections1 = int(FRCNN_1_detections.pop('num_detections'))\n",
        "  FRCNN_1_detections = {key: value[0, :num_detections1].numpy()\n",
        "                for key, value in FRCNN_1_detections.items()}\n",
        "  FRCNN_1_detections['num_detections'] = num_detections1\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  FRCNN_1_detections['detection_classes'] = FRCNN_1_detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "\n",
        "  boxes = np.squeeze(FRCNN_1_detections['detection_boxes']).tolist() if len(FRCNN_1_detections['detection_boxes']) != 1 else [np.squeeze(FRCNN_1_detections['detection_boxes']).tolist()]\n",
        "  scores = np.squeeze(FRCNN_1_detections['detection_scores']).tolist() if len(FRCNN_1_detections['detection_scores']) != 1 else [np.squeeze(FRCNN_1_detections['detection_scores']).tolist()]\n",
        "  classes = np.squeeze(FRCNN_1_detections['detection_classes']).tolist() if len(FRCNN_1_detections['detection_classes']) != 1 else [np.squeeze(FRCNN_1_detections['detection_classes']).tolist()]\n",
        "\n",
        "  return boxes, scores, classes"
      ],
      "metadata": {
        "id": "ksU2yd5xBrYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def frcnn_2_prediction():\n",
        "  # input_tensor = np.expand_dims(image_np, 0)\n",
        "  FRCNN_2_detections = detect_fn2(input_tensor)\n",
        "\n",
        "  # All outputs are batches tensors.\n",
        "  # Convert to numpy arrays, and take index [0] to remove the batch dimension.\n",
        "  # We're only interested in the first num_detections.\n",
        "\n",
        "  num_detections2 = int(FRCNN_2_detections.pop('num_detections'))\n",
        "  FRCNN_2_detections = {key: value[0, :num_detections2].numpy()\n",
        "                for key, value in FRCNN_2_detections.items()}\n",
        "  FRCNN_2_detections['num_detections'] = num_detections2\n",
        "\n",
        "  # detection_classes should be ints.\n",
        "  FRCNN_2_detections['detection_classes'] = FRCNN_2_detections['detection_classes'].astype(np.int64)\n",
        "\n",
        "\n",
        "  boxes = np.squeeze(FRCNN_2_detections['detection_boxes']).tolist() if len(FRCNN_2_detections['detection_boxes']) != 1 else [np.squeeze(FRCNN_2_detections['detection_boxes']).tolist()]\n",
        "  scores = np.squeeze(FRCNN_2_detections['detection_scores']).tolist() if len(FRCNN_2_detections['detection_scores']) != 1 else [np.squeeze(FRCNN_2_detections['detection_scores']).tolist()]\n",
        "  classes = np.squeeze(FRCNN_2_detections['detection_classes']).tolist() if len(FRCNN_2_detections['detection_classes']) != 1 else [np.squeeze(FRCNN_2_detections['detection_classes']).tolist()]\n",
        "\n",
        "  return boxes, scores, classes\n"
      ],
      "metadata": {
        "id": "vKHUlfcZFXm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ensemble_boxes import weighted_boxes_fusion\n",
        "\n",
        "def ensemble(boxes, scores, classes, weights):\n",
        "    boxes = [[[box[1], box[0], box[3], box[2]] for box in model] for model in boxes] # convert to xyxy format\n",
        "\n",
        "    iou_thr = 0.5\n",
        "    skip_box_thr = 0.0001\n",
        "    boxes, scores, classes = weighted_boxes_fusion(boxes, scores, classes, weights=weights, iou_thr=iou_thr,\n",
        "                                                   skip_box_thr=skip_box_thr)\n",
        "\n",
        "    boxes = np.squeeze(boxes).tolist() if len(boxes) != 1 else [np.squeeze(boxes).tolist()]\n",
        "    scores = np.squeeze(scores).tolist() if len(scores) != 1 else [np.squeeze(scores).tolist()]\n",
        "    classes = np.squeeze(classes).tolist() if len(classes) != 1 else [np.squeeze(classes).tolist()]\n",
        "\n",
        "    return boxes, scores, classes"
      ],
      "metadata": {
        "id": "V30senXIGtGT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def potholes_localization_pipeline():\n",
        "  v8_boxes, v8_scores, severities = yolov8_prediction()\n",
        "  v5_boxes, v5_scores, classes = yolov5_prediction()\n",
        "  f1_boxes, f1_scores, f1_classes = frcnn_1_prediction()\n",
        "  f2_boxes, f2_scores, f2_classes = frcnn_2_prediction()\n",
        "\n",
        "  boxes = [v8_boxes, v5_boxes, f1_boxes, f2_boxes]\n",
        "  scores = [v8_scores, v5_scores, f1_scores, f2_scores]\n",
        "  all_classes = [severities, classes, f1_classes, f2_classes]\n",
        "\n",
        "  all_classes = [[1 for cls in clss] for clss in all_classes]\n",
        "\n",
        "  boxes, scores, _ = ensemble(boxes, scores, all_classes, [2, 1, 1, 1])\n",
        "\n",
        "\n",
        "  return boxes, scores, classes, severities"
      ],
      "metadata": {
        "id": "qDhiGndoBYbF"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimension Estimation Module"
      ],
      "metadata": {
        "id": "1S7wZsy2I2rn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CALIBRATED_FOCAL_LENGTH = 10  # Calculate the focal length as described in the provided write-up (The provided number is just for the demo)\n",
        "CAMERA_ELEVATION_FROM_GROUND = 90  # We assumed that the camera is at 90cm elevation."
      ],
      "metadata": {
        "id": "FuRN417_Jdb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will calculate the pothole width and length and areal with the following equation.\n",
        "# But first we have to find pixel width and length for the pothole\n",
        "\n",
        "# pothole_width = pixel_width * CAMERA_ELEVATION_FROM_GROUND / CALIBRATED_FOCAL_LENGTH"
      ],
      "metadata": {
        "id": "tLjLiyKBKMqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_true_dimension(pixel_dim):\n",
        "  return pixel_dim * CAMERA_ELEVATION_FROM_GROUND / CALIBRATED_FOCAL_LENGTH"
      ],
      "metadata": {
        "id": "hCyGcEYmWvKO"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pothole_dimensions(boxes):\n",
        "  width_list = []\n",
        "  length_list = []\n",
        "  area_list = []\n",
        "\n",
        "  for box in boxes:\n",
        "    width_list.append(box[2] - box[0])\n",
        "    length_list.append(box[1] - box[3])\n",
        "\n",
        "  width_list = list(map(get_true_dimension, width_list))\n",
        "  length_list = list(map(get_true_dimension, length_list))\n",
        "\n",
        "  area_list = [w * h for w, h in zip(width_list, length_list)]\n",
        "\n",
        "\n",
        "  return width_list, length_list, area_list"
      ],
      "metadata": {
        "id": "NvdxLd1OKnoD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Severity Classification Pipeline"
      ],
      "metadata": {
        "id": "PAVY2RdcMEr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights should be set be domain experts and are provided here just for the demo\n",
        "\n",
        "CLASS_WEIGHT_MAP = {'wheel_mark': 0.2,\n",
        "                    'equal_interval': 0.2,\n",
        "                    'pavement': 0.5,\n",
        "                    'pothole': 1}\n",
        "\n",
        "SEVERITY_WEIGHT_MAP = {'crack': 0.7,\n",
        "                       'insignificant':0.8,\n",
        "                       'tolerable':0.9,\n",
        "                       'serious':1}"
      ],
      "metadata": {
        "id": "vOAFKBX5M7Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def harmonic_ranking(area, class_weight, severity_weight):\n",
        "  return 3 /((1/area) + (1/class_weight) + (1/severity_weight))"
      ],
      "metadata": {
        "id": "_f_xztwpTL9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def severity_scores_pipeline():\n",
        "  boxes, _, classes, severities = potholes_localization_pipeline()\n",
        "  _, _, area_list = get_pothole_dimensions(boxes)\n",
        "\n",
        "  classes_weights = list(map(lambda x: CLASS_WEIGHT_MAP[CLASSES_MAP[x]], classes))\n",
        "  severities_weights = list(map(lambda x: SEVERITY_WEIGHT_MAP[SEVERITY_CLASSES_MAP[x]], severities))\n",
        "\n",
        "  potholes_properties = zip(area_list, classes_weights, severities_weights)\n",
        "  \n",
        "  severity_scores = list(map(harmonic_ranking, potholes_properties))\n",
        "\n",
        "  return list(zip(boxes, severity_scores))"
      ],
      "metadata": {
        "id": "6eLgnev2MIX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call severity_scores_pipeline after loading the models and the image to get the severity score (out of 1) for each pothole in the image"
      ],
      "metadata": {
        "id": "QNGCqbA0Uth0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "severity_scores = severity_scores_pipeline()  # This returns each box coordinate with its severity score"
      ],
      "metadata": {
        "id": "zXnAiDlEU6el"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}